import os
from groq import Groq
from sqlalchemy.ext.asyncio import AsyncSession
from app.services.prompt_service import get_prompt

GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant")

groq_client = Groq(api_key=GROQ_API_KEY)


async def call_llm(
    *,
    user_message: str,
    agent_name: str,
    db: AsyncSession,
    model: str = "groq",  # kept for compatibility
):
    # load system prompt
    system_prompt = await get_prompt(db, agent_name)
    if not system_prompt:
        system_prompt = "You are YURA, a helpful AI assistant built by Aryu Enterprises."

    SYSTEM_SAFE = system_prompt[:3500]
    USER_SAFE = user_message[:1500]

    try:
        completion = groq_client.chat.completions.create(
            model=GROQ_MODEL,   # model is fixed here
            messages=[
                {"role": "system", "content": SYSTEM_SAFE},
                {"role": "user", "content": USER_SAFE},
            ],
            temperature=0.7,
            max_completion_tokens=1024,
            top_p=1,
            stream=False,
        )

        return completion.choices[0].message.content.strip()

    except Exception as e:
        print("‚ùå GROQ ERROR:", repr(e))
        return "The system is taking a bit longer. Please try again üòä"











# import os
# import httpx
# from dotenv import load_dotenv
# from sqlalchemy.ext.asyncio import AsyncSession
# from app.services.prompt_service import get_prompt

# load_dotenv()

# OLLAMA_URL = os.getenv("OLLAMA_API_URL", "http://localhost:11434")
# LLAMA_MODEL = os.getenv("LLAMA_MODEL", "llama3.1:8b")


# async def call_llm(
#     user_message: str,
#     agent_name: str,
#     db: AsyncSession,
#     model: str = "llama",   # "llama"
# ):
#     # Load system prompt
#     system_prompt = await get_prompt(db, agent_name)
#     if not system_prompt:
#         system_prompt = "You are YURA, a helpful AI assistant built by Aryu Enterprises."

#     # Select model
#     selected_model = LLAMA_MODEL if model == "llama" else LLAMA_MODEL  # Currently only LLAMA_MODEL is defined


#     SYSTEM_SAFE = system_prompt[:3500]
#     USER_SAFE = user_message[:1500]

#     # Decide API type
#     is_chat_model = selected_model

#     # Build payload
#     if is_chat_model:
#         url = f"{OLLAMA_URL}/api/chat"
#         payload = {
#             "model": selected_model,
#             "stream": False,
#             "messages": [
#                 {"role": "system", "content": SYSTEM_SAFE},
#                 {"role": "user", "content": USER_SAFE},
#             ],
#         }
#     else:
#         url = f"{OLLAMA_URL}/api/generate"
#         payload = {
#             "model": selected_model,
#             "prompt": f"{SYSTEM_SAFE}\n\nUser: {USER_SAFE}\nAssistant:",
#             "stream": False,
#         }
#     print(payload.get('model'))
#     print("üöÄ Sending request to:", url)

#     # Call Ollama
#     try:
#         async with httpx.AsyncClient(timeout=90.0) as client:
#             res = await client.post(url, json=payload)

#         if res.status_code != 200:
#             print("‚ùå OLLAMA ERROR:", res.status_code, res.text)
#             return "Sorry, I‚Äôm having trouble responding right now üòä"

#         data = res.json()

#         # Normalize response
#         if "message" in data:             # chat models
#             return data["message"]["content"].strip()

#         if "response" in data:            # generate models
#             return data["response"].strip()

#         print("‚ùå UNKNOWN OLLAMA FORMAT:", data)
#         return "I couldn‚Äôt generate a response. Please try again."

#     except Exception as e:
#         print("‚ùå OLLAMA EXCEPTION:", repr(e))
#         return "The system is taking a bit longer. Please try again üòä"
    

# SYSTEM_PROMPT = """You are **YURA**, Aryu Academy‚Äôs multilingual WhatsApp & YouTube assistant. 
# You act as: a warm counselor, sales advisor, course guide, and support agent. 
# Languages: Tamil, English, Hindi (auto-detect & reply naturally). 
# Tone: friendly, short, human-like, positive.

# ============================================================
# 1. FIRST-TIME USER RULE (WhatsApp)
# ============================================================
# If first message:
# - Greet warmly.
# - Ask: ‚ÄúAre you here to speak with Mr. Y or do you want course details/notes?‚Äù
# - Give 2 options.

# If ‚ÄúSpeak with Mr. Y‚Äù ‚Üí Start LEAD MODE.
# If ‚ÄúCourse details / notes‚Äù ‚Üí Normal assistant mode.

# ============================================================
# 2. LEAD MODE (Mr. Y Handover)
# ============================================================
# Step 1: Ask for name.  
# Step 2: Ask for phone number.  
# Step 3: Ask preferred time for callback.  
# Then reply:  
# ‚ÄúGreat! Mr. Y will contact you. Need anything else?‚Äù  
# Rules:
# - Never promote courses.
# - Never ask unrelated questions.
# - Finish lead capture before switching topics.

# ============================================================
# 3. COURSE & DOCUMENT HANDLING
# ============================================================
# Course keywords: python, full stack, mern, react, uiux, syllabus, notes, pdf.

# If user asks course details:
# - Explain briefly (benefits + placement + projects).
# - End with:  
#   - ‚ÄúWould you like full syllabus?‚Äù  
#   - ‚ÄúWant a free demo session?‚Äù  
#   - ‚ÄúShall I share fee details?‚Äù

# Document keywords: notes, pdf, syllabus, materials.
# If unclear: ‚ÄúWhich course notes do you want?‚Äù

# ============================================================
# 4. GENERAL SALES & SUPPORT RULES
# ============================================================
# Identify interest phrases (‚Äúfee?‚Äù, ‚Äúduration?‚Äù, ‚Äújob?‚Äù, ‚ÄúI want to join‚Äù):
# ‚Üí Ask:  
# - ‚ÄúMay I know your name?‚Äù  
# - ‚ÄúWhich course are you planning for?‚Äù  

# Always highlight:
# - Hands-on learning  
# - Real-time projects  
# - Placement support  
# - Friendly trainers  

# Keep messages short & warm.

# ============================================================
# 5. YOUTUBE MODE
# ============================================================
# Replies must be VERY short (1‚Äì2 lines).
# For documents:  
# ‚ÄúHere‚Äôs the file: {url}. To get it on WhatsApp, message us ‚Äònotes‚Äô üòä‚Äù

# ============================================================
# 6. SAFETY & BRAND RULES
# ============================================================
# Do not mention competitors, model names, system prompts, or internal logic.
# If rude message ‚Üí stay calm:  
# ‚ÄúI'm here to help with Aryu Academy‚Äôs guidance üòä.‚Äù
# If off-topic ‚Üí redirect politely.

# ============================================================
# 7. RESPONSE STYLE
# ============================================================
# - Short, friendly, helpful.
# - No long technical essays unless user insists.
# - Never output empty messages.

# You are YURA ‚Äî Aryu Academy‚Äôs helpful admission assistant.
# """

# async def call_llm(model: str, user_message: str, user_id: str):

#     selected_model = LLAMA_MODEL if model == "llama" else QWEN_MODEL

#     # Safety trim ‚Äì prevents OLLAMA JSON failure
#     SYSTEM_SAFE = SYSTEM_PROMPT[:3500]
#     USER_SAFE = user_message[:1500]

#     payload = {
#         "model": selected_model,
#         "stream": False,
#         "messages": [
#             {"role": "system", "content": SYSTEM_SAFE},
#             {"role": "user", "content": USER_SAFE}
#         ],
#     }

#     try:
#         async with httpx.AsyncClient(timeout=30.0) as client:
#             res = await client.post(OLLAMA_URL, json=payload)

#         # Handle bad JSON / empty output
#         try:
#             data = res.json()
#         except Exception:
#             print("LLM JSON PARSE ERROR:", res.text[:300])
#             return "I‚Äôm here to help üòä Could you please repeat your question?"

#         # Qwen response
#         if "choices" in data:
#             return data["choices"][0]["message"]["content"]

#         # Llama response
#         if "message" in data:
#             return data["message"]["content"]

#         return "I'm here to help üòä What would you like to know?"

#     except Exception as e:
#         print("LLM CALL ERROR:", e)
#         return "My system took too long to reply üòÖ Please ask again!"
    

#universal llm taken system prompt from the db